This directory contains source files for MHC-PRG. This readme describes how to set up MHC-PRG, using an existing graph for the xMHC.

We are currently testing the system on individuals with relatively high coverage (~ 30x). It may or may not work well on sample data with less coverage (we would certainly appreciate any feedback).

All relative paths specified in here are relative to the "src" directory of PnpHaplograph2.


Step 1: Set up the environment.

The pipeline requires Cortex (1.0.5.12), BWA (0.6.2-r126), Stampy (1.0.20), the PICARD_SAM2FASTQ from Picard (1.83), samtools (0.1.18), Platypus (0.2.0), Boost (1.41). The numbers in brackets indicate the version numbers as currently used for testing.

The pipeline also requires access to the human genome reference, a Cortex graph of the human genome reference, the PGF haplotype, and a haplotype graph of: the eight xMHC reference haplotypes, the HLA alleles and known SNPs. This latter graph is included with this package. For convenience, we also provide the other files in the required formats for download.

Open the file nextGenInferenceVariGraph.pl and modify the paths under "# external programs path" and "# external data paths" accordingly.

Open the makefile and modify the path to the Boost C++ library. A simple "make all" should then be sufficient to build the C++ components of the pipeline. If you run into problems, check that your compiler has support for openMP (we use G++ 4.6.3).

Step 2: Your sample data

For each sample, we require a BAM file and a Cortex graph, using the same k (kMer length) as the xMHC graph (this is usually 31). 

We use the BAM the re-map reads to the inferred xMHC "haplotypes", and we use the Cortex graph to extract kMer counts, which our haplotype graph model is based on.

Open nextGenInferenceVariGraph.pl and modify the lookup paths for these files, specified under "# data lookup paths".

For the Cortex graphs, the files should be named according to the convention: $sample_path.'/'.$sample.'_'.$kMer_size.'.ctx'

For the BAM files, the files should be named according to the convention: $base_path_BAMs.'/'.$individualID.'.bam'

Two give an example, with the paths used in the source distribution, we expect the following two files to be present for a sample named "AA02O9Q_A1":
	/Net/x9000/projects/gsk_hla/bam_output/AA02O9Q_A1.bam
	/ddn/projects3/mcvean_res/alex/CortexGraphs/AA02O9Q_A1_31.ctx

Step 3: Running the pipeline

Again, we assume the sample to be named "AA02O9Q_A1".

We probably need to increase the standard stack size to avoid memory problems. On Linux systems, the command 'ulimit -s 81920' should do.

Now execute

./nextGenInferenceVariGraph.pl --graph ../tmp2/GS_nextGen/varigraph3 --sample AA02O9Q_A1 --kmer 31

to prepare data analysis. If data preparation was successful, the script will print another command you need to execute. This will require some RAM (maybe 50 - 100G) and make use of openMP multithreading.

If you encounter an error, first check 'ulimit -a' and make sure that stack size was really set to 81920. If this was the case, send me an email.

Now execute three commands to generate data in different output formats:

./nextGenInferenceVariGraph.pl --graph ../tmp2/GS_nextGen/varigraph3 --sample AA02O9Q_A1 --kmer 31 --collect 2 --vcfPos ../data/VCFsnpRefData.txt ;\
./nextGenInferenceVariGraph.pl --graph ../tmp2/GS_nextGen/varigraph3 --sample AA02O9Q_A1 --kmer 31 --collect 2viterbi --vcfPos ../data/VCFsnpRefData.txt ;\
./nextGenInferenceVariGraph.pl --graph ../tmp2/GS_nextGen/varigraph3 --sample AA02O9Q_A1 --kmer 31 --collect 3 --vcfPos ../data/VCFsnpRefData.txt

The "collect" parameter decides what data exactly is collected and what output is produced.

"--collect 2" collects data that was generated by sampling from the posterior distribution over haplotype paths through the haplotype graph. The output file ../tmp/kMerCount__GS_nextGen_varigraph3_AA02O9Q_A1_31_required.binaryCount.haplotypes.VCF will have quality measures attached to alleles which are generated from taking the marginal allele probabilities at each position independently.

"--collect 2viterbi" collects data that was generated by calculating the Viterbi path through the haplotype graph model. The name of the output file  is. ../tmp/kMerCount__GS_nextGen_varigraph3_AA02O9Q_A1_31_required.binaryCount.viterbiHaplotypes.viterbiVCF.

"--collect 3" takes the Viterbi-generated haplotypes and carries out read re-mapping and de novo variant calling on top of the inferred haplotypes. Specifically, we create two "personalized reference genomes", by taking the original reference genome, removing the xMHC and inserting our two "haplotypes". We then (independently) re-map all sample reads to both "personalized reference genomes". We examine the "personalized xMHC" in both genomes. For each read in the union of reads that map to the "personalized xMHC" in either "personalized reference genome", we want to make a decision which of the genomes it "comes from". We use mapping quality to make this decision. When mapping quality is equal for both genomes, we make a random decision. We produce 2 SAM files, one for each "personalized xMHC", containing the reads we assigned to each "haplotype". We then carry out variant discovery (currently using Platypus), and modify the Viterbi haplotype VCF to take into account the variants discovered by Platypus. 

Step 4: Beyond the xMHC

See the file "README_otherREGIONS.txt" for more information on how to create your own graphs for other genomic regions than the xMHC.

TODO:

pack pgf files and GRch37.6 files, make path to human genome project accessible, and insert link into file, cortex genome graph of human genome



